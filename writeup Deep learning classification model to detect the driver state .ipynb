{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning classification model to detect drivers state :\n",
    "---\n",
    "Every day about 13 people in the Saudi Arabia are killed in crashes.People who use their cell phones to talk or text while driving are by far the most common reason for distracted driving accidents. \n",
    "\n",
    ">In this project our task was to get a dataset to build an deep learning classification model on. We chose to analyze a dataset obtained from kaggle that collects an images for the drivers state .\n",
    ">Goals:\n",
    "Build Deep learning classification model to predict the state of the driver .\n",
    "\n",
    "## Data Collection :\n",
    "---\n",
    ">**We chose the State Farm Distracted Driver Detection dataset for this project:**\n",
    ">- The dataset was obtained from  this website https://www.kaggle.com/c/state-farm-distracted-driver-detection/data \n",
    ">- the dataset contains 10 classes each class contains around 2000 images .\n",
    "\n",
    " \n",
    "## Prepossessing:\n",
    "---\n",
    "#### Dataset Before:\n",
    "Training dataset: 10 classes 22,024 images\n",
    "\n",
    "Test dataset: 2400 unlabelled images\n",
    "\n",
    "#### Dataset After:\n",
    "Training dataset: 3 classes 19624 images\n",
    "\n",
    "Validation dataset: 3 classes 2400 images\n",
    "\n",
    "Test dataset: 3 classes 2400 images\n",
    "\n",
    "- Image size:150*150\n",
    "- Reshape  \n",
    "## EDA graph:\n",
    "---\n",
    "\n",
    ">This graph shows the number of images for each class in the training set.\n",
    "<img src=\"EDA Graph_2.png\">\n",
    "\n",
    "\n",
    "## Modeling:\n",
    "----\n",
    "\n",
    "###   Baseline models:\n",
    "\n",
    "<img src=\"Baselin_ models.png\">\n",
    "###  CNN models:\n",
    "\n",
    "<img src=\"CNN_models.png\">\n",
    "\n",
    "###  Best Models:\n",
    "\n",
    "<img src=\"Best_Models.png\">\n",
    "\n",
    "### Best Model (VGG16) :\n",
    "<img src=\"Best Model (VGG16).png\">\n",
    "\n",
    "#### Results:\n",
    "Train Accuracy: 0.998618 \n",
    "\n",
    "Train Precision: 0.995899 \n",
    "\n",
    "Recall: 0.998999 \n",
    "\n",
    "F1 score: 0.997436 \n",
    "\n",
    "Test Accuracy: 0.930417 \n",
    "\n",
    "Test Precision: 0.933343 \n",
    "\n",
    "Test Recall: 0.929644 \n",
    "\n",
    "Test F1 score: 0.930894 \n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "\n",
    "## Conclusion:\n",
    "---\n",
    "Most models give us a good results in the both training and validation set and no overfitting .\n",
    "VGG16 shows best result in the test set . \n",
    "\n",
    "## Future work:\n",
    "---\n",
    "- Collect more images from different angle.\n",
    "- Tuning the models (different number of units and layers change the values of the hyperparameter )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
